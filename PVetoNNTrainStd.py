# -*- coding: utf-8 -*-
"""PVetoNNTrainStd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17kxCbUQO8iGo6y1O98LuWf62EsCmcQL9

PVeto Neural Network
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
trainingsigfile = ("/content/drive/My Drive/PVetoNN/PVetoMCData/Standard/StandardTraining/200913PVetoMCTrainingNoisySignalsLandauAmp.txt")
validationsigfile = ("/content/drive/My Drive/PVetoNN/PVetoMCData/Standard/StandardValidation/200913PVetoMCValidationNoisySignalsLandauAmp.txt")
print('trainingsigfile ',trainingsigfile)
print('validationsigfile ',validationsigfile)
trainingsignals = pd.read_csv(trainingsigfile, delimiter="\n",header=None)
validationsignals = pd.read_csv(validationsigfile, delimiter="\n",header=None)
print('trainingsignals.shape = ',trainingsignals.shape)
print('validationsignals.shape = ',validationsignals.shape)

trainingsignals = np.array(trainingsignals)
NTrainEvents = int(len(trainingsignals)/1024)
trainingsignals = trainingsignals.reshape(NTrainEvents,1024) # Instead of having shape (1024*NEvents,1), reshape to have shape (NEvents,1024)
print(trainingsignals.shape)

validationsignals = np.array(validationsignals)
NValidateEvents = int(len(validationsignals)/1024)
validationsignals = validationsignals.reshape(NValidateEvents,1024) # Instead of having shape (1024*NEvents,1), reshape to have shape (NEvents,1024)
print(validationsignals.shape)

import tensorflow as tf
# import label data
traininglabfile = "/content/drive/My Drive/PVetoNN/PVetoMCData/Standard/StandardTraining/200913PVetoMCTrainingHitsLandauAmp.txt"
validationlabfile = "/content/drive/My Drive/PVetoNN/PVetoMCData/Standard/StandardValidation/200913PVetoMCValidationHitsLandauAmp.txt"
print("traininglabfile ",traininglabfile)
print("validationlabfile ",validationlabfile)
traininglabels = np.loadtxt(traininglabfile)
validationlabels = np.loadtxt(validationlabfile)
print(traininglabels.shape)
print(validationlabels.shape)

X_train = trainingsignals
X_validate = validationsignals

Y_train = traininglabels
Y_validate = validationlabels

no_outputs = 14 # maximum number of hits expected

# force the labels to have 14 binary digits, one for each of the possible outputs 
Y_train=tf.one_hot(Y_train,no_outputs)
Y_validate=tf.one_hot(Y_validate,no_outputs)

print(X_train.shape)
print(X_validate.shape)
print(Y_train.shape)
print(Y_validate.shape)

# Lets visualise some events
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
from matplotlib.ticker import MultipleLocator

fig, axarr = plt.subplots(nrows=2, ncols=3)

# plot 3 events to check their content  
for index, event, title in zip(np.arange(6), trainingsignals[0:6], ['Event 0', 'Event 1', 'Event 2', 'Event 3', 'Event 4', 'Event 5']):# loop over zip(tuples)
    #print(int(index/3),index%3)
    ax=axarr[int(index/3),index%3]
    img = ax.plot(event)
    ax.set_title(title, fontsize=16)
    ax.tick_params(labelsize=12)
    ax.xaxis.set_major_locator(MultipleLocator(1))
    ax.xaxis.set_major_locator(MultipleLocator(100))
    fig.set_figwidth(20)
    fig.set_figheight(10)
    fig.tight_layout()

plt.show()

print(traininglabels[0:6])

fig, ax = plt.subplots(figsize=(20, 10))
ax.plot(trainingsignals[1][350:400])
plt.xticks(np.arange(0,50,step=1))  # Set label locations.
plt.grid(True)

from tensorflow import keras

# Following the architecture of the CNN from the image recognition lab (14/5/2020):
# Simple CNN:

# Define input to neural network (tensors of 1024 time samples x 1 amplitude per sample)
inputs = keras.Input(shape=(1024,1))
x=inputs

# 1st convolutional block
x = keras.layers.Conv1D(16, kernel_size=(3), name='Conv_1')(x)
x = keras.layers.LeakyReLU(0.1)(x)      
x = keras.layers.MaxPool1D((2), name='MaxPool_1')(x)

# 2nd convolutional block
x = keras.layers.Conv1D(16, kernel_size=(3), name='Conv_2')(x)
x = keras.layers.LeakyReLU(0.1)(x)
x = keras.layers.MaxPool1D((2), name='MaxPool_2')(x)

# 3rd convolutional block 
x = keras.layers.Conv1D(32, kernel_size=(3), name='Conv_3')(x)
x = keras.layers.LeakyReLU(0.1)(x)
x = keras.layers.MaxPool1D((2), name='MaxPool_3')(x)

# Flatten output tensor of the last convolutional layer so it can be used as  
# input to the dense layers

x = keras.layers.Flatten(name='Flatten')(x)

# dense network: 2 dense hidden layer with 256 neurons, with ReLU activation

# Classifier
x = keras.layers.Dense(64, name='Dense_1')(x)
x = keras.layers.ReLU(name='ReLU_dense_1')(x)
x = keras.layers.Dropout(0.2)(x)
x = keras.layers.Dense(64, name='Dense_2')(x)
x = keras.layers.ReLU(name='ReLU_dense_2')(x)


# Clusters are distributed according to a poisson distribution.
# In single particle mode this means no. clusters ~ Poi(1).
# The probability of having more than 4 clusters in single particle mode is 0.37%
outputs = keras.layers.Dense(no_outputs, activation='softmax', name='Output')(x)

# Model definition
model = keras.Model(inputs=inputs, outputs=outputs, name='VGGlike_CNN')

# Print model summary
model.summary()

# Show model structure
keras.utils.plot_model(model, show_shapes=True)

#Compilazione del Modello

# Definizione dei parametri di training del modello:
# * Loss function: 
# * Optimizer:
# * Metric: 

# Parametri:

# learning rate per Adam
LR_ST=1e-3

# Ottimizzatore: 
#adam: adaptive moment estimation (SGD con learning adattivo del gradiente e 
# del momento secondo + momentum )
#tf.keras.optimizers.Adam(
#    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,
#    name='Adam', **kwargs
#)

OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LR_ST)

# Loss function:
# We expect labels to be one-hot representation, then use CategoricalCrossentropy loss.

# Metric:
# Accuracy = "how often predictions equals labels... compute[s] the frequency with which y_pred matches y_true."

mae = tf.keras.metrics.MeanAbsoluteError()

# Eagerly RUN:
# di default Tensorflow ottimizza in modo spinto il modello che abbiamo progettato
# e trasforma il modello in un grafo diretto acicliclo computazionale altamente 
# ottimizzato per girare nel minor tempo possibile. Questo significa che non è 
# il codice python che abbiamo scritto sopra ad essere in realtà esegeuito. Questo
# può creare difficoltà con il debugging se ci sono problemi. Se sidesidera 
# eseguire esattamente il python scritto basta settare run_eagerly=True. Il prezzo
# da pagare è una esecuzioen estremamente più lenta (is usa solo in debug) 

EAGERLY = False

model.compile(optimizer=OPTIMIZER,
              loss='categorical_crossentropy',
              metrics=['accuracy'],
              run_eagerly=EAGERLY)

# TRAINING

# Parametr: frazione eventi validation set, batch size, numero epoche, ...

# Callbacks:
# tramite le callbacks possiamo interagire durante il run-time con il modello.
# Per esempio possiamo modificare il larnign rate facendolo decadere (diminuire) 
# nel tempo, o possiamo dire al modello di salvare i pesi ad ogni epoca o quando 
# la validation loss raggiunge il suo valore minimo etc..
# Le callback vengono chiamate all'inizio di ogni epoca o all'inizio di ogni 
# mini-batch a seconda dello scopo ...

# callback per avere un leraning rate variabile

# Definisco una funzione che mantiene il learning rate costante a LR_ST per le 
# prime 10 epoche e poi lo diminuisce esponenzialmente
def lr_decay(epoch):
  if epoch < 10:
    return LR_ST
  else:
    return LR_ST * tf.math.exp(0.2 * (10 - epoch))

lr_scheduler = keras.callbacks.LearningRateScheduler(lr_decay)

modelfilename='/content/drive/My Drive/PVetoNN/StandardModel/PVetoLandauAmpModel'
# callback per salvare il modello (solo i pesi in queto caso) ad ogni epoca
model_checkpoint = keras.callbacks.ModelCheckpoint(
        filepath = modelfilename,
        monitor='val_accuracy',
        save_weights_only=True, 
        save_best_only=True,
        save_freq='epoch')

# metto tutte le callback in una lista
callbacks = [ lr_scheduler, model_checkpoint ]    

print('type(Y_train) = ',type(Y_train.shape))

history = model.fit(X_train, Y_train, epochs=50,
                    validation_data=(X_validate,Y_validate), shuffle=True, verbose=1,
                    callbacks=callbacks)

#checks accuracy and loss on test sample
val_loss, val_accuracy = model.evaluate(X_validate,  Y_validate, verbose=2)
print('\nValidation loss (MSE):', val_loss)
print('\nValidation accuracy:', val_accuracy)

#plot della loss e della accuracy durante il training

accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs_range = range(1, len(loss) + 1)

plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, accuracy, label='Training accuracy')
plt.plot(epochs_range, val_accuracy, label='Validation accuracy')
plt.legend(loc='lower right')
plt.yticks(np.arange(0, 1, 0.1))
plt.title('Training and Validation accuracy')

plt.subplot(1, 2, 2)

plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.yticks(np.arange(0, 1, 0.1))
plt.title('Training and Validation Loss')
plt.show()